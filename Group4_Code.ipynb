{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT, SIMPLE_MOVEMENT, RIGHT_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    def __init__(self, save_path='Logs'):\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_losses = []\n",
    "        self.epsilons = []\n",
    "\n",
    "        self.mean_ep_rewards = []\n",
    "        self.mean_ep_lengths = []\n",
    "        self.mean_ep_losses = []\n",
    "        self.mean_ep_times = []\n",
    "\n",
    "        self.ep_times = []\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.curr_rewards = 0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_loss = 0\n",
    "        self.ep_time_start = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss):\n",
    "        self.curr_rewards += reward\n",
    "        self.curr_ep_length += 1\n",
    "\n",
    "        if loss is not None:\n",
    "            self.curr_loss += loss\n",
    "\n",
    "    def log_episode(self):\n",
    "        self.rewards.append(self.curr_rewards)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        self.ep_losses.append(self.curr_loss)\n",
    "\n",
    "        time_delta = time.time() - self.ep_time_start\n",
    "        self.ep_times.append(time_delta)\n",
    "\n",
    "        self.reset()\n",
    "    \n",
    "    def record(self, episode, step, epsilon, is_save=True):\n",
    "        mean_ep_reward = np.round(np.mean(self.rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_losses[-100:]), 3)\n",
    "        mean_ep_times = np.round(np.mean(self.ep_times[-100:]), 3)\n",
    "\n",
    "        self.epsilons.append(epsilon)\n",
    "\n",
    "        self.mean_ep_rewards.append(mean_ep_reward)\n",
    "        self.mean_ep_lengths.append(mean_ep_length)\n",
    "        self.mean_ep_losses.append(mean_ep_loss)\n",
    "\n",
    "        print(\n",
    "            f'Episode {episode} - '\n",
    "            f'Step {step} - '\n",
    "            f'Epsilon {epsilon} - '\n",
    "            f'MeanEpReward {mean_ep_reward} - '\n",
    "            f'MeanEpLength {mean_ep_length} - '\n",
    "            f'MeanEpLoss {mean_ep_loss} - '\n",
    "            f'MeanTime {mean_ep_times}'\n",
    "        )\n",
    "\n",
    "        if is_save:\n",
    "            with open(f'{self.save_path}/metrics.txt', 'a') as f:\n",
    "                f.write(\n",
    "                    f\"{episode:8d}{step:10d}{epsilon:20.8f}\"\n",
    "                    f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_times:15.3f}\\n\"\n",
    "                )\n",
    "\n",
    "                for metric in [\"mean_ep_rewards\", \"mean_ep_lengths\", 'mean_ep_times']:\n",
    "                    plt.plot(getattr(self, metric))\n",
    "                    plt.xlabel('Episodes')\n",
    "                    plt.ylabel(metric)\n",
    "                    plt.savefig(f\"{self.save_path}/{metric}_plot.jpg\")\n",
    "                    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import time, datetime\n",
    "import numpy as np\n",
    "from gym import ObservationWrapper\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "class FrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env=None, frames=1, limit=False, render_game=False):\n",
    "        super(FrameSkip, self).__init__(env)\n",
    "        self.frames = frames\n",
    "        self.tic = 0\n",
    "        self.limit = limit\n",
    "        self.render_game = render_game\n",
    "\n",
    "    def step(self, action):\n",
    "        net_reward = 0\n",
    "        for _ in range(self.frames):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            net_reward += reward\n",
    "            self.toc = time.perf_counter()\n",
    "\n",
    "            if self.render_game:\n",
    "                self.render()\n",
    "\n",
    "            if self.tic!=0 and self.limit != False:\n",
    "                time.sleep(max(self.limit-(self.toc-self.tic),0))\n",
    "            self.tic = time.perf_counter()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return obs, net_reward, done, info\n",
    "\n",
    "class Rescale(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env=None, shape=84):\n",
    "        super(Rescale, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return Rescale.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "\n",
    "        if frame.size == 240 * 256 * 3:\n",
    "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "\n",
    "        #greyscale\n",
    "        r, g, b = img[:, :, 0], img[:, :, 1], img[:, :, 2]\n",
    "        img = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "\n",
    "        resized_screen = cv2.resize(img[40:222, :], (84, 84), interpolation=cv2.INTER_AREA) # crop extra information on the screen\n",
    "\n",
    "        resized_screen *= 1.0 / resized_screen.max()   # norm\n",
    "        resized_screen = np.reshape(resized_screen, [84, 84, 1])\n",
    "\n",
    "        return resized_screen\n",
    "    \n",
    "def apply_wrappers(env):\n",
    "    env = FrameSkip(env, frames=4, limit=1 / 150, render_game=True)\n",
    "    env = Rescale(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelDDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuelDDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc_value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.fc_advantage = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.value = nn.Sequential(nn.Linear(512, 1))\n",
    "\n",
    "        self.advantage = nn.Sequential(nn.Linear(512, n_actions))\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "\n",
    "        value = self.fc_value(conv_out)\n",
    "        advantage = self.fc_advantage(conv_out)\n",
    "\n",
    "        value = self.value(value)\n",
    "        advantage = self.advantage(advantage)\n",
    "\n",
    "        avg_advantage = torch.mean(advantage, dim=1, keepdim=True)\n",
    "        Q = value + advantage - avg_advantage\n",
    "\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_space, action_space):\n",
    "\n",
    "        # Define DQN Layers\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.experience = deque(maxlen=30000)\n",
    "\n",
    "        self.lr = 0.00025\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Using {}\".format(self.device))\n",
    "\n",
    "        self.main_nn = DuelDDQN(state_space, action_space).to(self.device)\n",
    "        self.target_nn = DuelDDQN(state_space, action_space).to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.main_nn.parameters(), lr=self.lr)\n",
    "        self.network_copy = 5000\n",
    "        self.step = 0\n",
    "\n",
    "        self.batch_size = 32\n",
    "        self.eps = 1\n",
    "        self.eps_min = 0.01\n",
    "        self.eps_decay = 0.9997\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        # Learning parameters\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device)\n",
    "\n",
    "    def save_experience(self, state, action, reward, state2, done):\n",
    "        self.experience.append((state.float(), action.float(), reward.float(), state2.float(), done.float()))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        self.step += 1\n",
    "        if random.random() < self.eps:\n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "\n",
    "        return torch.argmax(self.main_nn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "\n",
    "    def copy_model(self):\n",
    "        self.target_nn.load_state_dict(self.main_nn.state_dict())\n",
    "\n",
    "    def train(self):\n",
    "        if self.step % self.network_copy == 0 and self.step > 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if len(self.experience) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.experience, self.batch_size)\n",
    "        state = torch.tensor(np.array([el[0].squeeze(dim=0) for el in minibatch])).to(self.device)\n",
    "        action = torch.tensor(np.array([el[1].squeeze(dim=0) for el in minibatch])).to(self.device)\n",
    "        reward = torch.tensor(np.array([el[2].squeeze(dim=0) for el in minibatch])).to(self.device)\n",
    "        next_state = torch.tensor(np.array([el[3].squeeze(dim=0) for el in minibatch])).to(self.device)\n",
    "        done = torch.tensor(np.array([el[4].squeeze(dim=0) for el in minibatch])).to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        target = reward + torch.mul((self.gamma * self.target_nn(next_state).max(1).values.unsqueeze(1)), 1 - done)\n",
    "        current = self.main_nn(state).gather(1, action.long())\n",
    "\n",
    "        # Updating network\n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.eps = max(self.eps, self.eps_min)\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    def save_model(self, episode, path):\n",
    "        torch.save(self.main_nn.state_dict(), f'{path}/episode_{episode}.pth')\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.main_nn.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = Path('logs')\n",
    "try:\n",
    "    save_dir.mkdir(parents=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = apply_wrappers(env)\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "agent = Agent(state_space=env.observation_space.shape, action_space=env.action_space.n)\n",
    "\n",
    "num_episodes = 20000 + 1\n",
    "\n",
    "logger = Logger(save_dir)\n",
    "for ep_num in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.Tensor(np.array([state]))\n",
    "    total_reward = 0\n",
    "    episode_reward = []\n",
    "    completed_level = False\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        state_next, reward, done, info = env.step(int(action[0]))\n",
    "\n",
    "        completed_level = info['flag_get']\n",
    "        total_reward += reward\n",
    "        episode_reward.append(reward) \n",
    "\n",
    "        # Stuck\n",
    "        if np.sum(episode_reward[-50:]) < 0:\n",
    "            break\n",
    "\n",
    "        state_next = torch.Tensor(np.array([state_next]))\n",
    "        reward_tensor = torch.tensor([reward]).unsqueeze(0)\n",
    "\n",
    "        terminal = torch.tensor(np.array([int(done)])).unsqueeze(0)\n",
    "\n",
    "        agent.save_experience(state, action, reward_tensor, state_next, terminal)\n",
    "        loss = agent.train()\n",
    "\n",
    "        logger.log_step(reward, loss)\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    agent.eps *= agent.eps_decay\n",
    "    # total_rewards.append(total_reward)\n",
    "    logger.log_episode()\n",
    "\n",
    "    logger.record(episode=ep_num,\n",
    "                    epsilon=agent.eps,\n",
    "                    step = agent.step,\n",
    "                    is_save=True)\n",
    "\n",
    "    if ep_num % 500 == 0 and ep_num > 0:\n",
    "        agent.save_model(ep_num, save_dir)\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = apply_wrappers(env)\n",
    "env = JoypadSpace(env, RIGHT_ONLY)\n",
    "\n",
    "agent = Agent(state_space=env.observation_space.shape, action_space=env.action_space.n)\n",
    "agent.load_model(f'{save_dir}/episode_10428.pth')\n",
    "agent.eps = 0\n",
    "\n",
    "completed_level = False\n",
    "while not completed_level:\n",
    "    state = env.reset()\n",
    "    state = torch.Tensor(np.array([state]))\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.choose_action(state)\n",
    "        state_next, reward, done, info = env.step(int(action[0]))\n",
    "        \n",
    "        state_next = torch.Tensor(np.array([state_next]))\n",
    "        completed_level = info['flag_get']\n",
    "\n",
    "        state = state_next\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
